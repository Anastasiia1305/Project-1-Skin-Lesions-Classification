{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CADx_Challenge2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YfcsLN7oBMO"
      },
      "source": [
        "Mount the drive "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBwFJ-22tLR1",
        "outputId": "671656ff-cfc2-4506-d1e7-0fb5240158c8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnQObbt9oAg4"
      },
      "source": [
        "Installing a newer version for implementing sift"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Djxx23istwxK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5905d2f7-c4c4-41c2-a8c7-f43ab74a4356"
      },
      "source": [
        "!pip install opencv-contrib-python==4.4.0.44"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-contrib-python==4.4.0.44\n",
            "  Downloading opencv_contrib_python-4.4.0.44-cp37-cp37m-manylinux2014_x86_64.whl (55.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 55.7 MB 53 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python==4.4.0.44) (1.19.5)\n",
            "Installing collected packages: opencv-contrib-python\n",
            "  Attempting uninstall: opencv-contrib-python\n",
            "    Found existing installation: opencv-contrib-python 4.1.2.30\n",
            "    Uninstalling opencv-contrib-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-contrib-python-4.1.2.30\n",
            "Successfully installed opencv-contrib-python-4.4.0.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8yxRzr3u4r8"
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from skimage.feature import local_binary_pattern\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, \\\n",
        "                            recall_score, accuracy_score, classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxSS09gip0dV"
      },
      "source": [
        "Extract Features \n",
        "1. Hu moments\n",
        "2. LBP\n",
        "3. color histogram\n",
        "\n",
        "Features Tried But did not work well\n",
        "4. HoG [Decrease in Accuracy]\n",
        "5. GLCM [Decrease in Accuracy]\n",
        "6. Haralick [Could not start mahotas.features ]\n",
        "\n",
        "Global Features\n",
        "7. SIFT [Could not combine local and global features using K-means]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUT0I3WFsWW0"
      },
      "source": [
        "def extract_hu_moments(img):\n",
        "    \"\"\"Extract Hu Moments feature of an image. Hu Moments are shape descriptors.\n",
        "    :param img: ndarray, BGR image\n",
        "    :return feature: ndarray, contains 7 Hu Moments of the image\n",
        "    \"\"\"\n",
        "    \n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    feature = cv2.HuMoments(cv2.moments(gray)).flatten()\n",
        "    return feature\n",
        "\n",
        "def extract_lbp(img, numPoints=24, radius=8):\n",
        "    \"\"\"Extract Local Binary Pattern histogram of an image. Local Binary Pattern features are texture descriptors.\n",
        "    :param img: ndarray, BGR image\n",
        "    :return feature: ndarray, contains (numPoints+2) Local Binary Pattern histogram of the image\n",
        "    \"\"\"\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    lbp = local_binary_pattern(gray, numPoints, radius, method='uniform')\n",
        "    n_bins = int(lbp.max() + 1)\n",
        "    feature, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)\n",
        "    return feature\n",
        "\n",
        "\n",
        "def extract_color_histogram(img, n_bins=8):\n",
        "    \"\"\"Extract Color histogram of an image.\n",
        "    :param img: ndarray, BGR image\n",
        "    :return feature: ndarray, contains n_bins*n_bins*n_bins HSV histogram features of the image\n",
        "    \"\"\"\n",
        "    \n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) # convert the image to HSV color-space\n",
        "    hist  = cv2.calcHist([hsv], [0, 1, 2], None, [n_bins, n_bins, n_bins], [0, 180, 0, 256, 0, 256])\n",
        "    cv2.normalize(hist, hist)\n",
        "    feature = hist.flatten()\n",
        "    return feature\n",
        "\n",
        "##Extracting the global features\n",
        "def extract_global_features(img):\n",
        "    \"\"\"Extract global features (shape, texture and color features) of an image.\n",
        "    :param img: ndarray, BGR image\n",
        "    :return global_feature: ndarray, contains shape, texture and color features of the image\n",
        "    \"\"\"\n",
        "    \n",
        "    hu_moments = extract_hu_moments(img)\n",
        "    #zernike_moments = extract_zernike_moments(img)\n",
        "    #haralick   = extract_haralick(img)\n",
        "    lbp_histogram  = extract_lbp(img)\n",
        "    color_histogram  = extract_color_histogram(img)\n",
        "    global_feature = np.hstack([hu_moments, lbp_histogram, color_histogram])\n",
        "    \n",
        "    return global_feature\n",
        "\n",
        "#### Attempt at extracting the local features but not successfull \n",
        "\n",
        "# def extract_keypoints(keypoint_detector, image):    \n",
        "#     keypoints, descriptors = keypoint_detector.detectAndCompute(image, None)\n",
        "#     return np.array(descriptors) if descriptors is not None else np.array([])\n",
        "\n",
        "\n",
        "# def flatten_keypoint_descriptors(X_train_local_features):\n",
        "#     descriptor_list_train = np.array(X_train_local_features[0])\n",
        "#     for remaining in X_train_local_features[1:]:\n",
        "#     \tdescriptor_list_train = np.vstack((descriptor_list_train, remaining))\n",
        "#     return descriptor_list_train\n",
        "\n",
        "\n",
        "# def cluster_local_features(descriptor_list_train, n_clusters=20): \n",
        "#     kmeans = KMeans(n_clusters=n_clusters)\n",
        "#     kmeans.fit(descriptor_list_train)\n",
        "#     return kmeans\n",
        "    \n",
        "\n",
        "# def extract_local_features(X_train_local_features, X_test_local_features):\n",
        "    \n",
        "#     k=20\n",
        "\n",
        "#     # # Scaling descriptors\n",
        "#     # scaler = MinMaxScaler()\n",
        "#     # X_train_local_features = scaler.fit_transform(X_train_local_features)\n",
        "#     # X_test_local_features = scaler.fit_transform(X_test_local_features)\n",
        "    \n",
        "#     km = KMeans(n_clusters=k)\n",
        "#     kmeans = km.fit(X_train_local_features)\n",
        "    \n",
        "#     print(\"[INFO] Adding cluster features\")\n",
        "#     X_clustered_train = kmeans.predict(X_train_local_features)\n",
        "#     X_clustered_test  = kmeans.predict(X_test_local_features)\n",
        "\n",
        "#     return X_clustered_train, X_clustered_test\n",
        "\n",
        "#### 2nd Attempt at extracting the local features but did not work \n",
        "\n",
        "# def extract_local_features(X_train_local_features, X_test_local_features):\n",
        "    \n",
        "#     n_clusters=20\n",
        "\n",
        "#     # flatten keypoint_descriptors\n",
        "#     # descriptor_list_train = flatten_keypoint_descriptors(X_train_local_features)\n",
        "#     # descriptor_list_test = flatten_keypoint_descriptors(X_test_local_features)\n",
        " \n",
        "#     # cluster keypoint descriptors\n",
        "#     kmeans = cluster_local_features(X_train_local_features, n_clusters=n_clusters)\n",
        "#     descriptor_clustered_train = kmeans.predict(X_train_local_features)\n",
        "#     descriptor_clustered_test = kmeans.predict(X_train_local_features)\n",
        "\n",
        "#     # For each image, count number of keypoints in each cluster that the image has\n",
        "#     X_clustered_train = np.array([np.zeros(n_clusters) for i in range(len(X_train_local_features))])\n",
        "#     old_count = 0\n",
        "#     for i in range(len(X_train_local_features)):\n",
        "#     \tnb_descriptors = len(X_train_local_features[i])\n",
        "#     \tfor j in range(nb_descriptors):\n",
        "#     \t\tidx = descriptor_clustered_train[old_count+j]\n",
        "#     \t\tX_clustered_train[i][idx] += 1\n",
        "#     \told_count += nb_descriptors\n",
        "    \n",
        "#     X_clustered_test = np.array([np.zeros(n_clusters) for i in range(len(X_test_local_features))])\n",
        "#     old_count = 0\n",
        "#     for i in range(len(X_test_local_features)):\n",
        "#         nb_descriptors = len(X_test_local_features[i])\n",
        "#         for j in range(nb_descriptors):\n",
        "#             idx = descriptor_clustered_test[old_count+j]\n",
        "#             X_clustered_test[i][idx] += 1\n",
        "#         old_count += nb_descriptors\n",
        "        \n",
        "#     return X_clustered_train, X_clustered_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSGcAsxaqkUU"
      },
      "source": [
        "Fitting the PCA algorithm with our extended features did not give promising results, therefore not using in final version of classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GI0uToxFqjc9"
      },
      "source": [
        "#### Fitting the PCA algorithm \n",
        "\n",
        "# def show_variance_explained_pca(data):\n",
        "#     \"\"\"\n",
        "#     Showing cumulative variance explained by the principle components after performing pca on input dataframe\n",
        "#     \"\"\"\n",
        "#     pca = PCA().fit(data)\n",
        "#     #Plotting the Cumulative Summation of the Explained Variance\n",
        "#     plt.figure()\n",
        "#     plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "#     plt.xlabel('Number of Components')\n",
        "#     plt.ylabel('Variance (%)') #for each component\n",
        "#     plt.title('Explained Variance')\n",
        "#     plt.show()\n",
        "    \n",
        "# def fit_pca(pca, train_x, val_x):\n",
        "#     \"\"\"\n",
        "#     Returning the transformed datasets using the provided pca\n",
        "#     Fitting using only training data, transforms both\n",
        "#     \"\"\"\n",
        "#     train_x_pca = pca.fit_transform(train_x)\n",
        "#     val_x_pca = pca.transform(val_x)    \n",
        "#     return train_x_pca, test_x_pca\n",
        "\n",
        "\n",
        "# pca = PCA().fit(train_data)\n",
        "# plt.figure()\n",
        "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "# plt.xlabel('Number of Components')\n",
        "# plt.ylabel('Variance (%)') #for each component\n",
        "# plt.title('Explained Variance')\n",
        "# plt.show()\n",
        "# train_x_pca = pca.fit_transform(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eINw3s_ZvKQ7"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, os.getcwd()) # add current working directory to pythonpath\n",
        "\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import linear_model\n",
        "import pickle\n",
        "import warnings\n",
        "import argparse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu1JRZulvO2d"
      },
      "source": [
        "def resize_image(img, img_size):\n",
        "    \"\"\"Pad and Resize image to a fixed size (img_size, img_size)\n",
        "    :param img: ndarray, BGR image\n",
        "    :param img_size: int, size to resize images.\n",
        "    :return img_resized: padded and resized image\n",
        "    \"\"\"\n",
        "    \n",
        "    # pad image to the max dimension\n",
        "    top = (max(img.shape[:2]) - img.shape[0])//2\n",
        "    bottom = max(img.shape[:2]) - img.shape[0] - top\n",
        "    left = (max(img.shape[:2]) - img.shape[1])//2\n",
        "    right = max(img.shape[:2]) - img.shape[1] - left\n",
        "    img_padded = cv2.copyMakeBorder(img, top, bottom, left, right,cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
        "    \n",
        "    # resize image\n",
        "    img_resized = cv2.resize(img_padded, (img_size, img_size))\n",
        "    return img_resized\n",
        "\n",
        "#### Removing hair from the provided images using blackhat\n",
        "def remove_hair(image):\n",
        "  # Convert the original image to grayscale\n",
        "  grayScale = cv2.cvtColor( image, cv2.COLOR_RGB2GRAY ) \n",
        "  #cv2_imshow(grayScale)\n",
        "\n",
        "  # Kernel for the morphological filtering\n",
        "  kernel = cv2.getStructuringElement(1,(17,17))\n",
        "\n",
        "  # clahe = cv2.createCLAHE(clipLimit = 5)\n",
        "  # clahe_img = clahe.apply(grayScale) + 30\n",
        "\n",
        "  # Perform the blackHat filtering on the grayscale image to find the hair countours\n",
        "  blackhat = cv2.morphologyEx(grayScale, cv2.MORPH_BLACKHAT, kernel)\n",
        "  #cv2_imshow(blackhat)\n",
        "\n",
        "  # intensify the hair countours in preparation for the inpainting  \n",
        "  # algorithm \n",
        "  ret,thresh2 = cv2.threshold(blackhat,10,255,cv2.THRESH_BINARY)\n",
        "  #cv2_imshow(thresh2)\n",
        "\n",
        "  # inpaint the original image depending on the mask\n",
        "  dst = cv2.inpaint(image,thresh2,1,cv2.INPAINT_TELEA)\n",
        "  #cv2_imshow(dst)\n",
        "\n",
        "  return dst\n",
        "\n",
        "#### Cycle through the folders, get labels and read each image, resize > remove hair > extract local and global features \n",
        "\n",
        "    \n",
        "def prepare_dataset(input_path, img_size=None, keypoint_detector=None):\n",
        "    \"\"\"Process images of different classes and extract labels and global features of images\n",
        "    :param input_path: str, path to the a dataset folder, which should have the below structure:\n",
        "        input_path\n",
        "            |---class_1\n",
        "            |---class_2\n",
        "            ...\n",
        "            |---class_N\n",
        "    : img_size: int, size to resize images. If None, resize will not be performed.\n",
        "    :return global_features: list, contains global features of images\n",
        "    :return label: list, contains labels (or classes) of images\n",
        "    \"\"\"\n",
        "    \n",
        "    # if keypoint_detector is None:\n",
        "    #     keypoint_detector = cv2.SIFT_create()\n",
        " \n",
        "    global_features = []\n",
        "    local_features = []\n",
        "    labels = []\n",
        "    folder_list = os.listdir(input_path)\n",
        "    for folder in folder_list:\n",
        "        print('Processing: ' + folder)       \n",
        "        folder_path = os.path.join(input_path, folder)\n",
        "        file_list = os.listdir(folder_path)\n",
        "        for filename in file_list:\n",
        "            img = cv2.imread(os.path.join(folder_path, filename))   #[:, :, :3] # ignore alpha channel\n",
        "            if img_size is not None:\n",
        "                img = resize_image(img, img_size)\n",
        "                img = remove_hair(img)\n",
        "            global_feature = extract_global_features(img)\n",
        "            global_features.append(global_feature)\n",
        "            labels.append(folder)\n",
        "            \n",
        "            #keypoint_descriptors = extract_keypoints(keypoint_detector, img)\n",
        "            #local_features.append(keypoint_descriptors)\n",
        "                   \n",
        "    return np.array(global_features), local_features, np.array(labels) \n",
        "    \n",
        "\n",
        "def train_model(model, X_train, y_train, parameters, n_splits=3):\n",
        "    \"\"\"Train model with Grid-search cross validation to find the best hyperparameter\n",
        "    :param model: Scikit-learn estimator\n",
        "    :param X_train: trainset features\n",
        "    :param y_train: trainset label\n",
        "    :param parameters: dict, key is hyper parameter name and value is a list of hyper parameter values\n",
        "    :return best_estimator: Scikit-learn estimator with the best hyper parameter\n",
        "    :return best_score: best accuracy score\n",
        "    :return best_param: dict, best hyper parameter\n",
        "    \"\"\"\n",
        "    \n",
        "    splits = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0).split(X=X_train, y=y_train)\n",
        "    \n",
        "    clf = GridSearchCV(model, parameters, cv=splits, scoring=make_scorer(accuracy_score))\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')  # disable the warning on default optimizer\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "    return clf.best_estimator_, clf.best_score_, clf.best_params_\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"Evaluate model on testset\n",
        "    :param model: Scikit-learn estimator\n",
        "    :param X_train: trainset features\n",
        "    :param y_train: trainset label\n",
        "    :param X_test: testset features\n",
        "    :param y_test: testset label\n",
        "    :param parameters: dict, key is hyper parameter name and value is a list of hyper parameter values\n",
        "    :return model: Scikit-learn estimator, fitted on the whole trainset\n",
        "    :return y_predict: prediction on test set\n",
        "    :return scores: dict, evaluation metrics on test set\n",
        "    \"\"\"\n",
        "    \n",
        "    # Refit the model on the whole train set\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter('ignore')  # disable the warning on default optimizer\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "    # Evaluate on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "    scores = None\n",
        "    if y_test is not None:\n",
        "        with warnings.catch_warnings():\n",
        "            warnings.simplefilter('ignore')  # disable the warning on f1-score with not all labels\n",
        "            #scores = get_prediction_score(y_test, y_predict)\n",
        "            print('*** TEST SET PERFORMANCE EVALUATION - Segmentation + Feature Extraction + SVM ***')\n",
        "            # compute and plot performance metrics\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            scores = accuracy\n",
        "            val_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "            val_recall = recall_score(y_test, y_pred, average='weighted')\n",
        "            val_precision = precision_score(y_test, y_pred, average='weighted')\n",
        "            val_kappa = cohen_kappa_score(y_test,y_pred,weights='quadratic')\n",
        "\n",
        "            print('Accuracy: {:.3f}'.format(accuracy))\n",
        "            print('Kappa: {:.3f}'.format(val_kappa))\n",
        "            print('F1-score: {:.3f}'.format(val_f1))\n",
        "            print('Recall: {:.3f}'.format(val_recall))\n",
        "            print('Precision: {:.3f}'.format(val_precision))\n",
        "    return model, y_pred, scores\n",
        "\n",
        "\n",
        "def build_base_models(X_train, y_train):\n",
        "    \"\"\"Train and evaluate different base models. \"Base\" means the model is not a stacking model. \n",
        "    :param X_train: trainset features\n",
        "    :param y_train: trainset label\n",
        "    :return fitted_models: list, contains fitted Scikit-learn estimators\n",
        "    :return model_names: list, names of fitted Scikit-learn estimators\n",
        "    :return model_scores: list, contains scores on test set for fitted Scikit-learn estimators.\n",
        "                    Each score is a dict of evaluation metrics\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    #### DEFINE BASE MODELS \n",
        "\n",
        "    models = []\n",
        "    model_params = []\n",
        "    model_names = []\n",
        "    \n",
        "    # Random forest model\n",
        "    for n_estimators in [500, 1000, 2000]:\n",
        "        for max_depth in [3, 5, 7]:\n",
        "            models.append(RandomForestClassifier(max_features='sqrt', class_weight='balanced', random_state=0))\n",
        "            model_params.append({'n_estimators':[n_estimators], 'max_depth':[max_depth]})\n",
        "            model_names.append('Random Forest')   \n",
        "    \n",
        "    # Boosted Tree\n",
        "    for n_estimators in [500, 1000, 2000]:\n",
        "        for max_depth in [3, 5, 7]:\n",
        "            for learning_rate in [0.01, 0.1]:\n",
        "                models.append(GradientBoostingClassifier(subsample=0.7, max_features='sqrt', random_state=0))\n",
        "                model_params.append({'n_estimators':[n_estimators], 'max_depth':[max_depth], 'learning_rate':[learning_rate]})\n",
        "                model_names.append('Gradient Boosting Machine')\n",
        "    \n",
        "    # SVM\n",
        "    for kernel in ['linear', 'rbf']:\n",
        "        for C in [1.0, 10.0, 100.0, 1000.0]:\n",
        "            models.append(SVC(probability=True, gamma='auto', tol=0.001, cache_size=200, class_weight='balanced',\n",
        "                              random_state=0,\n",
        "                              decision_function_shape='ovr'))\n",
        "            model_params.append({'kernel':[kernel], 'C':[C]})\n",
        "            model_names.append('Support Vector Machine')\n",
        "    \n",
        "    # Logistic regression model\n",
        "    for penalty in ['l1', 'l2']:\n",
        "        for C in [1.0, 10.0, 100.0, 1000.0]:\n",
        "            models.append(linear_model.LogisticRegression(max_iter=500, solver='liblinear', multi_class='ovr',\n",
        "                                                          class_weight='balanced', random_state=0))\n",
        "            model_params.append({'penalty':[penalty], 'C':[C]})\n",
        "            model_names.append('Logistic Regression')\n",
        "        \n",
        "    # KNN\n",
        "    for n_neighbors in [5, 10, 15]:\n",
        "        for weights in ['uniform', 'distance']:\n",
        "            models.append(KNeighborsClassifier())\n",
        "            model_params.append({'n_neighbors':[n_neighbors], 'weights':[weights]})\n",
        "            model_names.append('K Nearest Neighbour')\n",
        "            \n",
        "\n",
        "    #### TRAIN AND EVALUATE MODELS #\n",
        "\n",
        "    fitted_models = []\n",
        "    model_scores = []\n",
        "    for i in range(len(models)):\n",
        "        print('Evaluating model {} of {}: {}'.format((i+1), len(models), model_names[i]))\n",
        "        model = models[i]\n",
        "        fitted_cv, _, _ = train_model(model=model, X_train=X_train, y_train=y_train, parameters=model_params[i])\n",
        "        fitted_whole_set, _, score = evaluate_model(model=fitted_cv, X_train=X_train, y_train=y_train,\n",
        "                                                    X_test=X_test, y_test=y_test)\n",
        "        fitted_models.append(fitted_whole_set)\n",
        "        model_scores.append(score)\n",
        "        print(model_names[i], score)\n",
        "        \n",
        "    return fitted_models, model_names, model_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxrGRDnXtS0U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "467771e1-e748-4da6-f41b-a46dbcde4bb5"
      },
      "source": [
        "#Path for the data and output \n",
        "data_path = \"/drive/MyDrive/cad/cad2\"\n",
        "save_path = \"/drive/MyDrive/cad/cad2/output2\"\n",
        "\n",
        "#Variables \n",
        "img_size = None\n",
        "n_splits = 3\n",
        "\n",
        "# Extract features and labels for train set and test set\n",
        "train_path = \"/drive/MyDrive/CADx project /Challenges/cad/cad2/train (1)/train/train\"\n",
        "test_path = \"/drive/MyDrive/CADx project /Challenges/cad/cad2/val (1)/val/val\"\n",
        "\n",
        "#Extract global features for train and validation set \n",
        "X_train_global_features, keypoints_features_train,y_train = prepare_dataset(train_path, img_size=img_size)\n",
        "X_test_global_features, keypoints_features_test,y_test = prepare_dataset(test_path, img_size=img_size)\n",
        "\n",
        "#### Attempt at clustering the local features and combining them with global features \n",
        "#X_clustered_train, X_clustered_test = extract_local_features(keypoints_features_train,keypoints_features_test)\n",
        "# X_train_all_features = np.hstack((X_train_global_features, X_clustered_train))\n",
        "# X_test_all_features = np.hstack((X_test_global_features, X_clustered_test))\n",
        "\n",
        "#Change of names \n",
        "X_train_all_features = X_train_global_features\n",
        "X_test_all_features = X_test_global_features\n",
        "\n",
        "# Normalize features\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train = scaler.fit_transform(X_train_all_features)\n",
        "X_test = scaler.transform(X_test_all_features)\n",
        "\n",
        "# Build base models\n",
        "base_models, base_model_names, base_model_scores = build_base_models(X_train, y_train)\n",
        "if save_path is not None:\n",
        "    # Save base models    \n",
        "    os.makedirs(os.path.join(save_path, 'base_models'), exist_ok=True)\n",
        "    for i in range(len(base_models)):\n",
        "        with open(os.path.join(save_path, 'base_models', 'base_model_' + str(i+1) + '.pkl'), 'wb') as f:\n",
        "            pickle.dump(base_models[i], f)\n",
        "                    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7604c770fb1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#Extract global features for train and validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mX_train_global_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints_features_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mX_test_global_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeypoints_features_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-94c8905a6543>\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(input_path, img_size, keypoint_detector)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mlocal_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mfolder_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfolder_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Processing: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/drive/MyDrive/CADx project /Challenges/cad/cad2/train (1)/train/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQpbH8wL2izK",
        "outputId": "73b18823-a5f8-42ac-e9ec-399ded5121ea"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "#Set test folder path \n",
        "test_path = \"/drive/MyDrive/CADx project /Challenges/cad/cad2/test1\"\n",
        "\n",
        "#Set variables \n",
        "img_size = None\n",
        "global_features = []\n",
        "\n",
        "\n",
        "#Read images through the folder and extract features from them \n",
        "file_list = os.listdir(test_path)\n",
        "print(len(file_list))                             # Print the number of images present for later reference in prediction file \n",
        "for filename in file_list:\n",
        "  img = cv2.imread(os.path.join(test_path, filename))\n",
        "  if img_size is not None:\n",
        "    img = resize_image(img, img_size)\n",
        "    img = remove_hair(img)\n",
        "  test_feature = extract_global_features(img)\n",
        "  global_features.append(test_feature)\n",
        "           \n",
        "X_test_global_features = np.array(global_features)  \n",
        "print(np.array(X_test_global_features).shape)       # print the shape of the extracted features \n",
        "print(\"Feature Extration Complete\")\n",
        "\n",
        "# Normalize features\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "X_train = scaler.fit_transform(X_train_all_features)\n",
        "X_test = scaler.transform(X_test_global_features)\n",
        "\n",
        "# Best model path \n",
        "final_model = \"/drive/MyDrive/CADx project /Challenges/cad/cad2/output2/base_models/base_model_17.pkl\"\n",
        "\n",
        "## Load the best model and predict test data on the model \n",
        "loaded_model = pickle.load(open(final_model, 'rb'))\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "#Look at the predictions and change from name labels to designated labels \n",
        "print(y_pred)     \n",
        "y_pred = np.where(y_pred=='bcc', 0, y_pred)\n",
        "y_pred = np.where(y_pred=='bkl',1,y_pred)\n",
        "y_pred = np.where(y_pred=='mel',2,y_pred)\n",
        "print(y_pred)\n",
        "\n",
        "# Saving NumPy array as a csv file\n",
        "pd.DataFrame(y_pred).to_csv(\"/drive/MyDrive/CADx project /Challenges/cad/cad2/output2/challenge2_predictions.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226\n",
            "(226, 545)\n",
            "Feature Extration Complete\n",
            "['mel' 'mel' 'mel' 'mel' 'mel' 'bkl' 'mel' 'mel' 'bkl' 'mel' 'bkl' 'bkl'\n",
            " 'bkl' 'mel' 'mel' 'mel' 'mel' 'bkl' 'bkl' 'mel' 'mel' 'mel' 'mel' 'mel'\n",
            " 'mel' 'bkl' 'mel' 'mel' 'mel' 'mel' 'mel' 'bkl' 'bkl' 'mel' 'bcc' 'mel'\n",
            " 'mel' 'bkl' 'mel' 'bkl' 'mel' 'mel' 'bkl' 'mel' 'bkl' 'mel' 'bcc' 'bcc'\n",
            " 'mel' 'mel' 'bcc' 'mel' 'mel' 'bkl' 'bkl' 'bkl' 'mel' 'bcc' 'mel' 'mel'\n",
            " 'bkl' 'bkl' 'mel' 'bkl' 'mel' 'bkl' 'bkl' 'mel' 'mel' 'bkl' 'mel' 'mel'\n",
            " 'bkl' 'bkl' 'mel' 'mel' 'mel' 'mel' 'bkl' 'bkl' 'mel' 'mel' 'bcc' 'bkl'\n",
            " 'mel' 'bkl' 'mel' 'bkl' 'bkl' 'mel' 'bkl' 'bkl' 'bkl' 'mel' 'bkl' 'mel'\n",
            " 'bkl' 'mel' 'mel' 'mel' 'bkl' 'bkl' 'bcc' 'mel' 'bcc' 'bkl' 'mel' 'bkl'\n",
            " 'bkl' 'mel' 'bkl' 'mel' 'bkl' 'mel' 'mel' 'bcc' 'bkl' 'mel' 'mel' 'bkl'\n",
            " 'bkl' 'mel' 'bkl' 'mel' 'bkl' 'mel' 'mel' 'mel' 'bkl' 'mel' 'mel' 'bkl'\n",
            " 'bkl' 'bcc' 'bkl' 'bkl' 'bkl' 'bkl' 'bcc' 'bkl' 'bkl' 'bcc' 'mel' 'bkl'\n",
            " 'mel' 'bkl' 'mel' 'mel' 'mel' 'bkl' 'bkl' 'bkl' 'mel' 'bkl' 'bkl' 'mel'\n",
            " 'bkl' 'bkl' 'mel' 'mel' 'bkl' 'bkl' 'bkl' 'mel' 'mel' 'bkl' 'bcc' 'bkl'\n",
            " 'bkl' 'mel' 'bkl' 'bkl' 'mel' 'bkl' 'mel' 'bkl' 'bkl' 'bkl' 'mel' 'mel'\n",
            " 'mel' 'bkl' 'mel' 'bkl' 'bkl' 'bcc' 'mel' 'mel' 'bkl' 'bkl' 'mel' 'bkl'\n",
            " 'mel' 'mel' 'mel' 'bkl' 'bkl' 'bkl' 'mel' 'mel' 'mel' 'bkl' 'bcc' 'bkl'\n",
            " 'bkl' 'mel' 'mel' 'bkl' 'bkl' 'bkl' 'mel' 'bkl' 'mel' 'bkl' 'bkl' 'mel'\n",
            " 'mel' 'mel' 'mel' 'mel' 'mel' 'bkl' 'mel' 'bkl' 'mel' 'mel']\n",
            "['2' '2' '2' '2' '2' '1' '2' '2' '1' '2' '1' '1' '1' '2' '2' '2' '2' '1'\n",
            " '1' '2' '2' '2' '2' '2' '2' '1' '2' '2' '2' '2' '2' '1' '1' '2' '0' '2'\n",
            " '2' '1' '2' '1' '2' '2' '1' '2' '1' '2' '0' '0' '2' '2' '0' '2' '2' '1'\n",
            " '1' '1' '2' '0' '2' '2' '1' '1' '2' '1' '2' '1' '1' '2' '2' '1' '2' '2'\n",
            " '1' '1' '2' '2' '2' '2' '1' '1' '2' '2' '0' '1' '2' '1' '2' '1' '1' '2'\n",
            " '1' '1' '1' '2' '1' '2' '1' '2' '2' '2' '1' '1' '0' '2' '0' '1' '2' '1'\n",
            " '1' '2' '1' '2' '1' '2' '2' '0' '1' '2' '2' '1' '1' '2' '1' '2' '1' '2'\n",
            " '2' '2' '1' '2' '2' '1' '1' '0' '1' '1' '1' '1' '0' '1' '1' '0' '2' '1'\n",
            " '2' '1' '2' '2' '2' '1' '1' '1' '2' '1' '1' '2' '1' '1' '2' '2' '1' '1'\n",
            " '1' '2' '2' '1' '0' '1' '1' '2' '1' '1' '2' '1' '2' '1' '1' '1' '2' '2'\n",
            " '2' '1' '2' '1' '1' '0' '2' '2' '1' '1' '2' '1' '2' '2' '2' '1' '1' '1'\n",
            " '2' '2' '2' '1' '0' '1' '1' '2' '2' '1' '1' '1' '2' '1' '2' '1' '1' '2'\n",
            " '2' '2' '2' '2' '2' '1' '2' '1' '2' '2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8szFrlheG3Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}